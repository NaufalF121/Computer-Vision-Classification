{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-07T06:04:40.736627Z","iopub.execute_input":"2023-08-07T06:04:40.737035Z","iopub.status.idle":"2023-08-07T06:04:40.743287Z","shell.execute_reply.started":"2023-08-07T06:04:40.737006Z","shell.execute_reply":"2023-08-07T06:04:40.741761Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Data : https://www.kaggle.com/datasets/kritikseth/fruit-and-vegetable-image-recognition","metadata":{}},{"cell_type":"code","source":"import torch;\ntorch.__version__","metadata":{"execution":{"iopub.status.busy":"2023-08-07T06:04:40.745969Z","iopub.execute_input":"2023-08-07T06:04:40.746322Z","iopub.status.idle":"2023-08-07T06:04:40.760517Z","shell.execute_reply.started":"2023-08-07T06:04:40.746288Z","shell.execute_reply":"2023-08-07T06:04:40.759521Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'2.0.0'"},"metadata":{}}]},{"cell_type":"code","source":"!pip install imutils\n!pip install git+https://github.com/Deci-AI/super-gradients.git@stable\n!pip install albumentations \n!pip install split-folders[full]\n!pip install --upgrade -q wandb","metadata":{"execution":{"iopub.status.busy":"2023-08-07T06:04:40.761562Z","iopub.execute_input":"2023-08-07T06:04:40.761873Z","iopub.status.idle":"2023-08-07T06:07:15.857400Z","shell.execute_reply.started":"2023-08-07T06:04:40.761842Z","shell.execute_reply":"2023-08-07T06:07:15.856031Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting imutils\n  Downloading imutils-0.5.4.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: imutils\n  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25859 sha256=d02b45436c147e712dca61a6c632493c0f16dda01502e125de9b2d1759d925ba\n  Stored in directory: /root/.cache/pip/wheels/85/cf/3a/e265e975a1e7c7e54eb3692d6aa4e2e7d6a3945d29da46f2d7\nSuccessfully built imutils\nInstalling collected packages: imutils\nSuccessfully installed imutils-0.5.4\nCollecting git+https://github.com/Deci-AI/super-gradients.git@stable\n  Cloning https://github.com/Deci-AI/super-gradients.git (to revision stable) to /tmp/pip-req-build-5gm17lqx\n  Running command git clone --filter=blob:none --quiet https://github.com/Deci-AI/super-gradients.git /tmp/pip-req-build-5gm17lqx\n  Running command git checkout -q c1587c542dd67b11ed1c043024adf2c6ed479f22\n  Resolved https://github.com/Deci-AI/super-gradients.git to commit c1587c542dd67b11ed1c043024adf2c6ed479f22\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (2.0.0)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (4.65.0)\nRequirement already satisfied: boto3>=1.17.15 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (1.26.100)\nRequirement already satisfied: jsonschema>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (4.17.3)\nRequirement already satisfied: Deprecated>=1.2.11 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (1.2.14)\nRequirement already satisfied: opencv-python>=4.5.1 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (4.8.0.74)\nRequirement already satisfied: scipy>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (1.11.1)\nRequirement already satisfied: matplotlib>=3.3.4 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (3.7.1)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (5.9.3)\nRequirement already satisfied: tensorboard>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (2.12.3)\nRequirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (59.8.0)\nCollecting coverage~=5.3.1 (from super-gradients==3.1.3+master)\n  Downloading coverage-5.3.1.tar.gz (684 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m684.5/684.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torchvision>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (0.15.1)\nCollecting sphinx~=4.0.2 (from super-gradients==3.1.3+master)\n  Downloading Sphinx-4.0.3-py3-none-any.whl (2.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: sphinx-rtd-theme in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (0.2.4)\nCollecting torchmetrics==0.8 (from super-gradients==3.1.3+master)\n  Downloading torchmetrics-0.8.0-py3-none-any.whl (408 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.6/408.6 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting hydra-core>=1.2.0 (from super-gradients==3.1.3+master)\n  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting omegaconf (from super-gradients==3.1.3+master)\n  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting onnxruntime==1.13.1 (from super-gradients==3.1.3+master)\n  Downloading onnxruntime-1.13.1-cp310-cp310-manylinux_2_27_x86_64.whl (4.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting onnx==1.13.0 (from super-gradients==3.1.3+master)\n  Downloading onnx-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pillow!=8.3,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (9.5.0)\nCollecting pip-tools>=6.12.1 (from super-gradients==3.1.3+master)\n  Downloading pip_tools-7.2.0-py3-none-any.whl (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyparsing==2.4.5 (from super-gradients==3.1.3+master)\n  Downloading pyparsing-2.4.5-py2.py3-none-any.whl (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting einops==0.3.2 (from super-gradients==3.1.3+master)\n  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\nCollecting pycocotools==2.0.6 (from super-gradients==3.1.3+master)\n  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: protobuf==3.20.3 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (3.20.3)\nCollecting treelib==1.6.1 (from super-gradients==3.1.3+master)\n  Downloading treelib-1.6.1.tar.gz (24 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting termcolor==1.1.0 (from super-gradients==3.1.3+master)\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: packaging>=20.4 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (21.3)\nRequirement already satisfied: wheel>=0.38.0 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (0.40.0)\nRequirement already satisfied: pygments>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (2.15.1)\nCollecting stringcase>=1.2.0 (from super-gradients==3.1.3+master)\n  Downloading stringcase-1.2.0.tar.gz (3.0 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting numpy<=1.23 (from super-gradients==3.1.3+master)\n  Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: rapidfuzz in /opt/conda/lib/python3.10/site-packages (from super-gradients==3.1.3+master) (3.1.1)\nCollecting json-tricks==3.16.1 (from super-gradients==3.1.3+master)\n  Downloading json_tricks-3.16.1-py2.py3-none-any.whl (27 kB)\nCollecting onnx-simplifier<1.0,>=0.3.6 (from super-gradients==3.1.3+master)\n  Downloading onnx_simplifier-0.4.33-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.10/site-packages (from onnx==1.13.0->super-gradients==3.1.3+master) (4.6.3)\nCollecting coloredlogs (from onnxruntime==1.13.1->super-gradients==3.1.3+master)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime==1.13.1->super-gradients==3.1.3+master) (23.5.26)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime==1.13.1->super-gradients==3.1.3+master) (1.12)\nCollecting pyDeprecate==0.3.* (from torchmetrics==0.8->super-gradients==3.1.3+master)\n  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\nRequirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from treelib==1.6.1->super-gradients==3.1.3+master) (0.18.3)\nRequirement already satisfied: botocore<1.30.0,>=1.29.100 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.17.15->super-gradients==3.1.3+master) (1.29.161)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.17.15->super-gradients==3.1.3+master) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.17.15->super-gradients==3.1.3+master) (0.6.1)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from Deprecated>=1.2.11->super-gradients==3.1.3+master) (1.14.1)\nCollecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.2.0->super-gradients==3.1.3+master)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.2.0->super-gradients==3.1.3+master) (23.1.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.2.0->super-gradients==3.1.3+master) (0.19.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.4->super-gradients==3.1.3+master) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.4->super-gradients==3.1.3+master) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.4->super-gradients==3.1.3+master) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.4->super-gradients==3.1.3+master) (1.4.4)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.4->super-gradients==3.1.3+master) (2.8.2)\nRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from omegaconf->super-gradients==3.1.3+master) (6.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from onnx-simplifier<1.0,>=0.3.6->super-gradients==3.1.3+master) (13.4.2)\nCollecting build (from pip-tools>=6.12.1->super-gradients==3.1.3+master)\n  Downloading build-0.10.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: click>=8 in /opt/conda/lib/python3.10/site-packages (from pip-tools>=6.12.1->super-gradients==3.1.3+master) (8.1.3)\nRequirement already satisfied: pip>=22.2 in /opt/conda/lib/python3.10/site-packages (from pip-tools>=6.12.1->super-gradients==3.1.3+master) (23.1.2)\nRequirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from pip-tools>=6.12.1->super-gradients==3.1.3+master) (2.0.1)\nCollecting sphinxcontrib-applehelp (from sphinx~=4.0.2->super-gradients==3.1.3+master)\n  Downloading sphinxcontrib_applehelp-1.0.4-py3-none-any.whl (120 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sphinxcontrib-devhelp (from sphinx~=4.0.2->super-gradients==3.1.3+master)\n  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sphinxcontrib-jsmath (from sphinx~=4.0.2->super-gradients==3.1.3+master)\n  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\nCollecting sphinxcontrib-htmlhelp (from sphinx~=4.0.2->super-gradients==3.1.3+master)\n  Downloading sphinxcontrib_htmlhelp-2.0.1-py3-none-any.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.8/99.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sphinxcontrib-serializinghtml (from sphinx~=4.0.2->super-gradients==3.1.3+master)\n  Downloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sphinxcontrib-qthelp (from sphinx~=4.0.2->super-gradients==3.1.3+master)\n  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: Jinja2>=2.3 in /opt/conda/lib/python3.10/site-packages (from sphinx~=4.0.2->super-gradients==3.1.3+master) (3.1.2)\nCollecting docutils<0.18,>=0.14 (from sphinx~=4.0.2->super-gradients==3.1.3+master)\n  Downloading docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.5/575.5 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: snowballstemmer>=1.1 in /opt/conda/lib/python3.10/site-packages (from sphinx~=4.0.2->super-gradients==3.1.3+master) (2.2.0)\nRequirement already satisfied: babel>=1.3 in /opt/conda/lib/python3.10/site-packages (from sphinx~=4.0.2->super-gradients==3.1.3+master) (2.12.1)\nCollecting alabaster<0.8,>=0.7 (from sphinx~=4.0.2->super-gradients==3.1.3+master)\n  Downloading alabaster-0.7.13-py3-none-any.whl (13 kB)\nCollecting imagesize (from sphinx~=4.0.2->super-gradients==3.1.3+master)\n  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\nRequirement already satisfied: requests>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from sphinx~=4.0.2->super-gradients==3.1.3+master) (2.31.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->super-gradients==3.1.3+master) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->super-gradients==3.1.3+master) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->super-gradients==3.1.3+master) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->super-gradients==3.1.3+master) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->super-gradients==3.1.3+master) (3.4.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->super-gradients==3.1.3+master) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->super-gradients==3.1.3+master) (2.3.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->super-gradients==3.1.3+master) (3.12.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->super-gradients==3.1.3+master) (3.1)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3>=1.17.15->super-gradients==3.1.3+master) (1.26.15)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients==3.1.3+master) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients==3.1.3+master) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients==3.1.3+master) (4.9)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients==3.1.3+master) (1.16.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->super-gradients==3.1.3+master) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=2.3->sphinx~=4.0.2->super-gradients==3.1.3+master) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients==3.1.3+master) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients==3.1.3+master) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients==3.1.3+master) (2023.5.7)\nCollecting pyproject_hooks (from build->pip-tools>=6.12.1->super-gradients==3.1.3+master)\n  Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.13.1->super-gradients==3.1.3+master)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->onnx-simplifier<1.0,>=0.3.6->super-gradients==3.1.3+master) (2.2.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime==1.13.1->super-gradients==3.1.3+master) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->onnx-simplifier<1.0,>=0.3.6->super-gradients==3.1.3+master) (0.1.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients==3.1.3+master) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->super-gradients==3.1.3+master) (3.2.2)\nBuilding wheels for collected packages: super-gradients, pycocotools, termcolor, treelib, coverage, antlr4-python3-runtime, stringcase\n  Building wheel for super-gradients (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for super-gradients: filename=super_gradients-3.1.3+master-py3-none-any.whl size=1017328 sha256=b2580cfa6f1c2968983947a6187289867ec3f37a4c5fee4119626e4a3fbf0c50\n  Stored in directory: /tmp/pip-ephem-wheel-cache-5xp88t3z/wheels/54/e2/9b/fc2a50b22310d756211a0430c540ec7f2a8a593670e077eaa2\n  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp310-cp310-linux_x86_64.whl size=101736 sha256=1df1f3c175e5c96a17fd0507c948bffbc97233e409c8f5f4f1771e916a36f263\n  Stored in directory: /root/.cache/pip/wheels/58/e6/f9/f87c8f8be098b51b616871315318329cae12cdb618f4caac93\n  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=19f482e7a0ff03db2851d1b6a80ac28f46055ebb34ecbee99e838f960066e246\n  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n  Building wheel for treelib (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for treelib: filename=treelib-1.6.1-py3-none-any.whl size=18384 sha256=79c06d0d0a42358a59f9f02917004b887b88c250611c7068321a974b92a936a0\n  Stored in directory: /root/.cache/pip/wheels/63/72/8b/76569b82bf280a03c4e294c3b29ee2398217186369c427ed4b\n  Building wheel for coverage (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for coverage: filename=coverage-5.3.1-cp310-cp310-linux_x86_64.whl size=210093 sha256=db14ca30b93d760e9272d9e2fd73e01487372dc2103bac20b9342e5e896828ee\n  Stored in directory: /root/.cache/pip/wheels/e2/70/10/313be697f460d6024cfa94b7f0e22ffc1c53aab718fb4f42af\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144574 sha256=d1ff758c7a4c8665390bad79e73693dd52bdb85ab9e53fe92487fcf981eca775\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n  Building wheel for stringcase (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for stringcase: filename=stringcase-1.2.0-py3-none-any.whl size=3588 sha256=529d529fc262faf87de2a04288873aa772f1628f74404643354a1f7b89336b8c\n  Stored in directory: /root/.cache/pip/wheels/31/ba/22/1a2d952a9ce8aa86e42fda41e2c87fdaf20e238c88bf8df013\nSuccessfully built super-gradients pycocotools termcolor treelib coverage antlr4-python3-runtime stringcase\nInstalling collected packages: termcolor, stringcase, json-tricks, einops, antlr4-python3-runtime, treelib, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, pyproject_hooks, pyparsing, pyDeprecate, omegaconf, numpy, imagesize, humanfriendly, docutils, coverage, alabaster, onnx, coloredlogs, torchmetrics, sphinx, onnxruntime, onnx-simplifier, hydra-core, build, pycocotools, pip-tools, super-gradients\n  Attempting uninstall: termcolor\n    Found existing installation: termcolor 2.3.0\n    Uninstalling termcolor-2.3.0:\n      Successfully uninstalled termcolor-2.3.0\n  Attempting uninstall: pyparsing\n    Found existing installation: pyparsing 3.0.9\n    Uninstalling pyparsing-3.0.9:\n      Successfully uninstalled pyparsing-3.0.9\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Uninstalling numpy-1.23.5:\n      Successfully uninstalled numpy-1.23.5\n  Attempting uninstall: docutils\n    Found existing installation: docutils 0.20.1\n    Uninstalling docutils-0.20.1:\n      Successfully uninstalled docutils-0.20.1\n  Attempting uninstall: onnx\n    Found existing installation: onnx 1.14.0\n    Uninstalling onnx-1.14.0:\n      Successfully uninstalled onnx-1.14.0\n  Attempting uninstall: torchmetrics\n    Found existing installation: torchmetrics 1.0.0\n    Uninstalling torchmetrics-1.0.0:\n      Successfully uninstalled torchmetrics-1.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nchex 0.1.81 requires numpy>=1.25.0, but you have numpy 1.23.0 which is incompatible.\ncudf 23.6.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cuda 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.0 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.1 which is incompatible.\npytoolconfig 1.2.5 requires packaging>=22.0, but you have packaging 21.3 which is incompatible.\nraft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed alabaster-0.7.13 antlr4-python3-runtime-4.9.3 build-0.10.0 coloredlogs-15.0.1 coverage-5.3.1 docutils-0.17.1 einops-0.3.2 humanfriendly-10.0 hydra-core-1.3.2 imagesize-1.4.1 json-tricks-3.16.1 numpy-1.23.0 omegaconf-2.3.0 onnx-1.13.0 onnx-simplifier-0.4.33 onnxruntime-1.13.1 pip-tools-7.2.0 pyDeprecate-0.3.2 pycocotools-2.0.6 pyparsing-2.4.5 pyproject_hooks-1.0.0 sphinx-4.0.3 sphinxcontrib-applehelp-1.0.4 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.5 stringcase-1.2.0 super-gradients-3.1.3+master termcolor-1.1.0 torchmetrics-0.8.0 treelib-1.6.1\nRequirement already satisfied: albumentations in /opt/conda/lib/python3.10/site-packages (1.3.1)\nRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.23.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.11.1)\nRequirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.21.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from albumentations) (6.0)\nRequirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.0.4)\nRequirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from albumentations) (4.8.0.74)\nRequirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (1.2.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (4.6.3)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (3.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2.31.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2023.4.12)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\nRequirement already satisfied: lazy_loader>=0.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image>=0.16.1->albumentations) (2.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\nCollecting split-folders[full]\n  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from split-folders[full]) (4.65.0)\nInstalling collected packages: split-folders\nSuccessfully installed split-folders-0.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from PIL import Image\nimport os\nimport numpy as np\n\n\nimport random\nimport pathlib\nimport wandb\nimport math\nimport random\nfrom typing import Dict, List,Tuple\nimport requests\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nfrom pathlib import Path, PurePath\nimport pathlib\nimport pandas as pd\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision import transforms\n\nfrom PIL import Image\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom sklearn.model_selection import train_test_split\n\nfrom imutils import paths\n\nimport splitfolders\nimport textwrap\n\nimport super_gradients\nfrom super_gradients.common.object_names import Models\nfrom super_gradients.training import Trainer\nfrom super_gradients.training import training_hyperparams\nfrom super_gradients.training.metrics.classification_metrics import Accuracy, Top5\nfrom super_gradients.training.utils.early_stopping import EarlyStop\nfrom super_gradients.training import models\nfrom super_gradients.training.utils.callbacks import Phase\n\nimport splitfolders\nimport textwrap\nimport torch\nfrom torch.utils.data import Dataset\nimport json\nimport os\nfrom PIL import Image\nfrom pathlib import Path\nimport cv2\nimport matplotlib.pyplot as plt\nfrom glob import glob\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport tifffile as tif\nimport seaborn as sns\nfrom PIL import Image, ImageEnhance; Image.MAX_IMAGE_PIXELS = 5_000_000_000;\nimport matplotlib; print(f\"\\t\\t– MATPLOTLIB VERSION: {matplotlib.__version__}\");\nfrom matplotlib import animation, rc; rc('animation', html='jshtml')\nimport plotly\nimport PIL\nimport cv2\nimport tensorflow as tf\nimport plotly.io as pio\nprint(pio.renderers)\nimport gc\nimport requests\nimport torch\nfrom PIL import Image\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport tifffile as tif\nimport seaborn as sns\nfrom PIL import Image, ImageEnhance; Image.MAX_IMAGE_PIXELS = 5_000_000_000;\nimport matplotlib; print(f\"\\t\\t– MATPLOTLIB VERSION: {matplotlib.__version__}\");\nfrom matplotlib import animation, rc; rc('animation', html='jshtml')\nimport plotly\nimport PIL\nimport cv2\nimport shutil","metadata":{"execution":{"iopub.status.busy":"2023-08-07T06:08:06.196624Z","iopub.execute_input":"2023-08-07T06:08:06.197032Z","iopub.status.idle":"2023-08-07T06:08:06.215401Z","shell.execute_reply.started":"2023-08-07T06:08:06.197000Z","shell.execute_reply":"2023-08-07T06:08:06.214732Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\t\t– MATPLOTLIB VERSION: 3.7.2\nRenderers configuration\n-----------------------\n    Default renderer: 'kaggle'\n    Available renderers:\n        ['plotly_mimetype', 'jupyterlab', 'nteract', 'vscode',\n         'notebook', 'notebook_connected', 'kaggle', 'azure', 'colab',\n         'cocalc', 'databricks', 'json', 'png', 'jpeg', 'jpg', 'svg',\n         'pdf', 'browser', 'firefox', 'chrome', 'chromium', 'iframe',\n         'iframe_connected', 'sphinx_gallery', 'sphinx_gallery_png']\n\n\t\t– MATPLOTLIB VERSION: 3.7.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\n# I have saved my API token with \"wandb_api\" as Label. \n# If you use some other Label make sure to change the same below. \nwandb_api = user_secrets.get_secret(\"wandb_api\") \n\nwandb.login(key=wandb_api)\nwandb_name = wandb.util.generate_id()\nwandb.init(project=\"Fruit classification with regnetY800\", name=wandb_name)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-07T06:07:32.030817Z","iopub.execute_input":"2023-08-07T06:07:32.031541Z","iopub.status.idle":"2023-08-07T06:07:35.482719Z","shell.execute_reply.started":"2023-08-07T06:07:32.031498Z","shell.execute_reply":"2023-08-07T06:07:35.482031Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\t\t– MATPLOTLIB VERSION: 3.7.2\nRenderers configuration\n-----------------------\n    Default renderer: 'kaggle'\n    Available renderers:\n        ['plotly_mimetype', 'jupyterlab', 'nteract', 'vscode',\n         'notebook', 'notebook_connected', 'kaggle', 'azure', 'colab',\n         'cocalc', 'databricks', 'json', 'png', 'jpeg', 'jpg', 'svg',\n         'pdf', 'browser', 'firefox', 'chrome', 'chromium', 'iframe',\n         'iframe_connected', 'sphinx_gallery', 'sphinx_gallery_png']\n\n\t\t– MATPLOTLIB VERSION: 3.7.2\n","output_type":"stream"},{"name":"stderr","text":"Custom TB Handler failed, unregistering\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a list with the filepaths for training and testing\ntrain_dir = Path('/kaggle/input/fruit-and-vegetable-image-recognition/train')\ntrain_filepaths = list(train_dir.glob(r'**/*.jpg'))\n\ntest_dir = Path('/kaggle/input/fruit-and-vegetable-image-recognition/test')\ntest_filepaths = list(test_dir.glob(r'**/*.jpg'))\n\nval_dir = Path('/kaggle/input/fruit-and-vegetable-image-recognition/validation')\nval_filepaths = list(test_dir.glob(r'**/*.jpg'))\n\ndef proc_img(filepath):\n    \"\"\" Create a DataFrame with the filepath and the labels of the pictures\n    \"\"\"\n\n    labels = [str(filepath[i]).split(\"/\")[-2] \\\n              for i in range(len(filepath))]\n\n    filepath = pd.Series(filepath, name='Filepath').astype(str)\n    labels = pd.Series(labels, name='Label')\n\n    # Concatenate filepaths and labels\n    df = pd.concat([filepath, labels], axis=1)\n\n    # Shuffle the DataFrame and reset index\n    df = df.sample(frac=1).reset_index(drop = True)\n    \n    return df\n\ntrain_df = proc_img(train_filepaths)\ntest_df = proc_img(test_filepaths)\nval_df = proc_img(val_filepaths)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T06:08:11.323735Z","iopub.execute_input":"2023-08-07T06:08:11.324152Z","iopub.status.idle":"2023-08-07T06:08:13.100272Z","shell.execute_reply.started":"2023-08-07T06:08:11.324119Z","shell.execute_reply":"2023-08-07T06:08:13.099870Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print('-- Training set --\\n')\nprint(f'Jumlah Foto : {train_df.shape[0]}\\n')\nprint(f'Jumlah Label : {len(train_df.Label.unique())}\\n')\nprint(f'Label: {train_df.Label.unique()}')","metadata":{"execution":{"iopub.status.busy":"2023-08-07T06:08:15.328168Z","iopub.execute_input":"2023-08-07T06:08:15.328527Z","iopub.status.idle":"2023-08-07T06:08:15.340132Z","shell.execute_reply.started":"2023-08-07T06:08:15.328496Z","shell.execute_reply":"2023-08-07T06:08:15.339754Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"-- Training set --\n\nJumlah Foto : 2780\n\nJumlah Label : 36\n\nLabel: ['pomegranate' 'pineapple' 'kiwi' 'peas' 'lettuce' 'carrot' 'mango'\n 'paprika' 'raddish' 'apple' 'garlic' 'cauliflower' 'banana' 'beetroot'\n 'chilli pepper' 'spinach' 'watermelon' 'potato' 'turnip' 'ginger'\n 'sweetcorn' 'lemon' 'eggplant' 'sweetpotato' 'capsicum' 'cucumber'\n 'jalepeno' 'corn' 'onion' 'soy beans' 'bell pepper' 'orange' 'cabbage'\n 'tomato' 'pear' 'grapes']\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:36:54.315188Z","iopub.execute_input":"2023-08-05T12:36:54.317444Z","iopub.status.idle":"2023-08-05T12:36:54.337125Z","shell.execute_reply.started":"2023-08-05T12:36:54.317410Z","shell.execute_reply":"2023-08-05T12:36:54.336734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display some pictures of the dataset\nfig, axes = plt.subplots(nrows=6, ncols=6, figsize=(8, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(train_df.Filepath[i]))\n    ax.set_title(train_df.Label[i], fontsize = 12)\nplt.tight_layout(pad=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:36:54.343350Z","iopub.execute_input":"2023-08-05T12:36:54.343998Z","iopub.status.idle":"2023-08-05T12:37:04.771437Z","shell.execute_reply.started":"2023-08-05T12:36:54.343965Z","shell.execute_reply":"2023-08-05T12:37:04.770976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = []\nY = []","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:04.778545Z","iopub.execute_input":"2023-08-05T12:37:04.779283Z","iopub.status.idle":"2023-08-05T12:37:04.783411Z","shell.execute_reply.started":"2023-08-05T12:37:04.779220Z","shell.execute_reply":"2023-08-05T12:37:04.782886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transfer_between_folders(source1, source2, dest, split_rate):\n    \"\"\" Based on the split ratio this function moves some portion of the source folder to destination folder!\n\n        Args:\n            source: str\n                Source folder's path\n            dest: str\n                Destination folder's path\n            split_rate: float\n                Ratio of files to move from source to dest locaiton\n\n    \"\"\"\n    global source_files\n    source_files = os.listdir(source)\n    if(len(source_files) != 0):\n        transfer_file_numbers = int(len(source_files)*split_rate)\n        transfer_index = random.sample(\n            range(0, len(source_files)), transfer_file_numbers)\n        for each_index in transfer_index:\n            shutil.move(os.path.join(source, str(source_files[each_index])), os.path.join(\n                dest, str(source_files[each_index])))\n\n    else:\n        print(\"No file moved. Source empty!\")\n        \ndef transfer_all_class_between_folders(source1, source2, dest, split_rate):\n    \"\"\" Transfer the files from source to dest for all the classes. This function calls the 'transfer_between_folders' to actually perform the transfer.\n\n        Args:\n            source: str\n                Source folder's path\n            dest: str\n                Destination folder's path\n            split_rate: float\n                Ratio of files to move from source to dest locaiton\n\n    \"\"\"\n    for label in config.LABEL:\n        transfer_between_folders(os.path.join(source1, config.LABEL),\n                                 os.path.join(\n                                      source2, config.LABEL),\n                                 split_rate)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:04.784972Z","iopub.execute_input":"2023-08-05T12:37:04.785623Z","iopub.status.idle":"2023-08-05T12:37:04.797856Z","shell.execute_reply.started":"2023-08-05T12:37:04.785591Z","shell.execute_reply":"2023-08-05T12:37:04.797362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    # specify the paths to datasets\n    DATA_DIR = Path('/kaggle/input/fruit-and-vegetable-image-recognition')\n    ROOT_DIR = Path('/kaggle/working/')\n    TRAIN_DIR = \"/kaggle/input/fruit-and-vegetable-image-recognition/train\"\n    TEST_DIR = \"/kaggle/input/fruit-and-vegetable-image-recognition/test\"\n    VAL_DIR = '/kaggle/input/fruit-and-vegetable-image-recognition/validation'\n\n    # set the input height and width\n    INPUT_HEIGHT = 256\n    INPUT_WIDTH = 256\n\n    # set the input heig/ht and width\n    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n    IMAGENET_STD = [0.229, 0.224, 0.225]\n    \n    BATCH_SIZE = 128\n    MODEL_NAME = 'regnetY800'\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    TRAINING_PARAMS = 'training_hyperparams/default_train_params'\n    \n    NUM_CLASSES = len(train_df.Label.unique())\n    LABEL = train_df.Label.unique()\n\n    CHECKPOINT_DIR = 'checkpoints'\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:04.799444Z","iopub.execute_input":"2023-08-05T12:37:04.800589Z","iopub.status.idle":"2023-08-05T12:37:04.814636Z","shell.execute_reply.started":"2023-08-05T12:37:04.800533Z","shell.execute_reply":"2023-08-05T12:37:04.814301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize our data augmentation functions\nresize = transforms.Resize(size=(config.INPUT_HEIGHT,config.INPUT_WIDTH))\nmake_tensor = transforms.ToTensor()\nnormalize = transforms.Normalize(mean=config.IMAGENET_MEAN, std=config.IMAGENET_STD)\ncenter_cropper = transforms.CenterCrop((config.INPUT_HEIGHT,config.INPUT_WIDTH))\nrandom_horizontal_flip = transforms.RandomHorizontalFlip(p=0.75)\nrandom_vertical_flip = transforms.RandomVerticalFlip(p=0.75)\nrandom_rotation = transforms.RandomRotation(degrees=90)\nrandom_crop = transforms.RandomCrop(size=(200,200))\naugmix = transforms.AugMix(severity = 3, mixture_width=3, alpha=0.2)\nauto_augment = transforms.AutoAugment()\nrandom_augment = transforms.RandAugment()\n\n# initialize our training and validation set data augmentation pipeline\ntrain_transforms = transforms.Compose([\n  resize, \n  auto_augment,\n  augmix,\n  random_augment,\n  make_tensor,\n  normalize\n])\n\nval_transforms = transforms.Compose([resize, make_tensor, normalize])","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:04.844849Z","iopub.execute_input":"2023-08-05T12:37:04.846820Z","iopub.status.idle":"2023-08-05T12:37:04.857530Z","shell.execute_reply.started":"2023-08-05T12:37:04.846787Z","shell.execute_reply":"2023-08-05T12:37:04.857167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataloaders(\n    train_dir: str, \n    val_dir: str,\n    test_dir: str,\n    train_transform: transforms.Compose,\n    val_transform:  transforms.Compose,\n    test_transform:  transforms.Compose,\n    batch_size: int, \n    num_workers: int=2\n):\n  \n  # Use ImageFolder to create dataset\n  train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n  val_data = datasets.ImageFolder(val_dir, transform=val_transform)\n  test_data = datasets.ImageFolder(test_dir, transform=val_transform)  \n\n  print(f\"[INFO] training dataset contains {len(train_data)} samples...\")\n  print(f\"[INFO] validation dataset contains {len(val_data)} samples...\")\n  print(f\"[INFO] test dataset contains {len(test_data)} samples...\")\n\n  # Get class names\n  class_names = train_data.classes\n  print(f\"[INFO] dataset contains {len(class_names)} labels...\")\n\n  # Turn images into data loaders\n  print(\"[INFO] creating training and validation set dataloaders...\")\n  train_dataloader = DataLoader(\n      train_data,\n      batch_size=batch_size,\n      shuffle=True,\n      drop_last=True,\n      num_workers=num_workers,\n      pin_memory=True,\n      persistent_workers=True\n  )\n  val_dataloader = DataLoader(\n      val_data,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n      drop_last=False,\n      persistent_workers=True\n  )\n\n  test_dataloader = DataLoader(\n      test_data,\n      batch_size=batch_size,\n      shuffle=False,\n      num_workers=num_workers,\n      pin_memory=True,\n      drop_last=False,\n      persistent_workers=True\n  )\n\n  return train_dataloader, val_dataloader, test_dataloader, class_names","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:04.858794Z","iopub.execute_input":"2023-08-05T12:37:04.859434Z","iopub.status.idle":"2023-08-05T12:37:04.875537Z","shell.execute_reply.started":"2023-08-05T12:37:04.859367Z","shell.execute_reply":"2023-08-05T12:37:04.875164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader, valid_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=config.TRAIN_DIR,\n                                                                     val_dir=config.VAL_DIR,\n                                                                     test_dir=config.TEST_DIR,\n                                                                     train_transform=train_transforms,\n                                                                     val_transform=val_transforms,\n                                                                     test_transform=val_transforms,\n                                                                     batch_size=config.BATCH_SIZE)\n\nNUM_CLASSES = len(class_names)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:04.876555Z","iopub.execute_input":"2023-08-05T12:37:04.876953Z","iopub.status.idle":"2023-08-05T12:37:04.973582Z","shell.execute_reply.started":"2023-08-05T12:37:04.876918Z","shell.execute_reply":"2023-08-05T12:37:04.972099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_params =  training_hyperparams.get(config.TRAINING_PARAMS)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:04.977877Z","iopub.execute_input":"2023-08-05T12:37:04.980175Z","iopub.status.idle":"2023-08-05T12:37:05.614144Z","shell.execute_reply.started":"2023-08-05T12:37:04.980150Z","shell.execute_reply":"2023-08-05T12:37:05.613614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ntraining_params = {\n             # Your training params\n    \"sg_logger\": \"wandb_sg_logger\", # Weights&Biases Logger, see class super_gradients.common.sg_loggers.wandb_sg_logger.WandBSGLogger for details\n    \"sg_logger_params\":             # Params that will be passes to __init__ of the logger super_gradients.common.sg_loggers.wandb_sg_logger.WandBSGLogger\n      {\n        \"project_name\": \"Fruit classification with regnetY800\", # W&B project name\n        \"save_checkpoints_remote\": True,\n        \"save_tensorboard_remote\": True,\n        \"save_logs_remote\": True,\n        \"entity\": \"rpl\"   # Optional: In case your experiment tracking is not hosted at wandb servers\n      }\n    \n}\n# To reduce clutter in the notebook I've turned the verbosity off, you can turn it on to see the full output\nearly_stop_acc = EarlyStop(Phase.VALIDATION_EPOCH_END, monitor=\"Accuracy\", mode=\"max\", patience=7, verbose=True)\nearly_stop_val_loss = EarlyStop(Phase.VALIDATION_EPOCH_END, monitor=\"LabelSmoothingCrossEntropyLoss\", mode=\"min\", patience=7, verbose=True)\n\ntraining_params[\"train_metrics_list\"] = [Accuracy(), Top5()]\ntraining_params[\"valid_metrics_list\"] = [Accuracy(), Top5()]\n\n\n# Set the silent mode to True to reduce clutter in the notebook, you can turn it on to see the full output\ntraining_params[\"silent_mode\"] = False\ntraining_params[\"optimizer\"] = 'AdamW'\ntraining_params[\"criterion_params\"] = {'smooth_eps': 0.20}\ntraining_params[\"max_epochs\"] = 250\ntraining_params[\"initial_lr\"] = 0.0001\ntraining_params[\"loss\"] = \"cross_entropy\"","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:49:39.988090Z","iopub.execute_input":"2023-08-05T12:49:39.988824Z","iopub.status.idle":"2023-08-05T12:49:40.004538Z","shell.execute_reply.started":"2023-08-05T12:49:39.988779Z","shell.execute_reply":"2023-08-05T12:49:40.003981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(training_params)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:49:42.603521Z","iopub.execute_input":"2023-08-05T12:49:42.603909Z","iopub.status.idle":"2023-08-05T12:49:42.609773Z","shell.execute_reply.started":"2023-08-05T12:49:42.603877Z","shell.execute_reply":"2023-08-05T12:49:42.609293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.get(config.MODEL_NAME, num_classes = config.NUM_CLASSES, pretrained_weights='imagenet')","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:49:46.933749Z","iopub.execute_input":"2023-08-05T12:49:46.934141Z","iopub.status.idle":"2023-08-05T12:49:47.116124Z","shell.execute_reply.started":"2023-08-05T12:49:46.934109Z","shell.execute_reply":"2023-08-05T12:49:47.115592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_model_trainer = Trainer(experiment_name='Fruit classification with regnetY800', ckpt_root_dir=config.CHECKPOINT_DIR)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:49:48.687009Z","iopub.execute_input":"2023-08-05T12:49:48.687598Z","iopub.status.idle":"2023-08-05T12:49:48.696398Z","shell.execute_reply.started":"2023-08-05T12:49:48.687563Z","shell.execute_reply":"2023-08-05T12:49:48.695512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_model_trainer.train(model=model, \n              training_params=training_params, \n              train_loader=train_dataloader,\n              valid_loader=valid_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:49:50.373655Z","iopub.execute_input":"2023-08-05T12:49:50.374050Z","iopub.status.idle":"2023-08-05T12:52:57.391300Z","shell.execute_reply.started":"2023-08-05T12:49:50.374017Z","shell.execute_reply":"2023-08-05T12:52:57.390901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_full_model = models.get(config.MODEL_NAME,\n                        num_classes=config.NUM_CLASSES,\n                        checkpoint_path=os.path.join(full_model_trainer.checkpoints_dir_path, \"ckpt_best.pth\"))","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:50.117142Z","iopub.status.idle":"2023-08-05T12:37:50.118060Z","shell.execute_reply.started":"2023-08-05T12:37:50.117775Z","shell.execute_reply":"2023-08-05T12:37:50.117801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred_and_plot_image(image_path: str, \n                        subplot: Tuple[int, int, int],  # subplot tuple for `subplot()` function\n                        class_names: List[str] = class_names,\n                        model: torch.nn.Module = best_full_model,\n                        image_size: Tuple[int, int] = (config.INPUT_HEIGHT, config.INPUT_WIDTH),\n                        transform: torchvision.transforms = None,\n                        device: torch.device=config.DEVICE):\n\n    if isinstance(image_path, pathlib.PosixPath):\n      img = Image.open(image_path)\n    else: \n      img = Image.open(requests.get(image_path, stream=True).raw)\n\n    # create transformation for image (if one doesn't exist)\n    if transform is None:\n        transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=config.IMAGENET_MEAN,\n                                 std=config.IMAGENET_STD),\n        ])\n    transformed_image = transform(img)\n\n    # make sure the model is on the target device\n    model.to(device)\n\n    # turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n        # add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n        transformed_image = transformed_image.unsqueeze(dim=0)\n\n        # make a prediction on image with an extra dimension and send it to the target device\n        target_image_pred = model(transformed_image.to(device))\n\n    # convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # convert prediction probabilities -> prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n\n    # actual label\n    ground_truth = PurePath(image_path).parent.name\n\n    # plot image with predicted label and probability \n    plt.subplot(*subplot)\n    plt.imshow(img)\n    if isinstance(image_path, pathlib.PosixPath):\n        title = f\"Ground Truth: {ground_truth} | Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n    else:\n        title = f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n    plt.title(\"\\n\".join(textwrap.wrap(title, width=20)))  # wrap text using textwrap.wrap() function\n    plt.axis(False)\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:50.119763Z","iopub.status.idle":"2023-08-05T12:37:50.121614Z","shell.execute_reply.started":"2023-08-05T12:37:50.121346Z","shell.execute_reply":"2023-08-05T12:37:50.121371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = 'https://th.bing.com/th/id/R.ef9ff3be84f85e3ca222b8d9b16198b5?rik=DbNJVlgcB%2fV1LA&riu=http%3a%2f%2fcottageintheoaks.files.wordpress.com%2f2011%2f05%2fcorn-featured-vegetable.jpg&ehk=ZCXWFaYmBoNkuXISD7SebYWc1krNDd5ZDI4%2fGiA56Ug%3d&risl=&pid=ImgRaw&r=0'\npred_and_plot_image(image_path= image, subplot=(1, 1, 1))","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:50.125776Z","iopub.status.idle":"2023-08-05T12:37:50.126672Z","shell.execute_reply.started":"2023-08-05T12:37:50.126405Z","shell.execute_reply":"2023-08-05T12:37:50.126430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = 'https://www.gardeningknowhow.com/wp-content/uploads/2021/05/organic-pears-on-a-tree.jpg'\npred_and_plot_image(image_path= image, subplot=(1, 1, 1))","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:50.128207Z","iopub.status.idle":"2023-08-05T12:37:50.129081Z","shell.execute_reply.started":"2023-08-05T12:37:50.128805Z","shell.execute_reply":"2023-08-05T12:37:50.128830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = 'https://th.bing.com/th/id/R.f84b480a518fc98e28c3df89b3861954?rik=px288gPNXFvnJw&riu=http%3a%2f%2fstatic.republika.co.id%2fuploads%2fimages%2finpicture_slide%2fjahe-_140311172716-448.jpg&ehk=HQle2NHXKrfgV67BHSrZRbGQmsDU61RxBkOEkzaXt28%3d&risl=&pid=ImgRaw&r=0'\npred_and_plot_image(image_path= image, subplot=(1, 1, 1))","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:50.131702Z","iopub.status.idle":"2023-08-05T12:37:50.132544Z","shell.execute_reply.started":"2023-08-05T12:37:50.132278Z","shell.execute_reply":"2023-08-05T12:37:50.132303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = 'https://th.bing.com/th/id/OIP.hHu4F3X3vzArSEHQskL02AHaFj?w=249&h=187&c=7&r=0&o=5&dpr=1.3&pid=1.7'\npred_and_plot_image(image_path= image, subplot=(1, 1, 1))","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:50.133917Z","iopub.status.idle":"2023-08-05T12:37:50.134705Z","shell.execute_reply.started":"2023-08-05T12:37:50.134460Z","shell.execute_reply":"2023-08-05T12:37:50.134482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = 'https://i.pinimg.com/originals/48/15/28/4815281f9b9351265fbe2b0173d5aa4b.jpg'\npred_and_plot_image(image_path= image, subplot=(1, 1, 1))","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:50.136085Z","iopub.status.idle":"2023-08-05T12:37:50.136875Z","shell.execute_reply.started":"2023-08-05T12:37:50.136632Z","shell.execute_reply":"2023-08-05T12:37:50.136654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Set model to evaluation mode\nbest_full_model.eval()\n\n# Create empty lists to store true labels and predicted labels\ntrue_labels = []\npredicted_labels = []\n\n# Loop over batches in test dataloader, make predictions, and append true and predicted labels to lists\nfor images, labels in test_dataloader:\n    images = images.to(config.DEVICE)\n    labels = labels.to(config.DEVICE)\n    with torch.no_grad():\n        outputs = best_full_model(images)\n        _, predicted = torch.max(outputs.data, 1)\n    true_labels.extend(labels.cpu().numpy())\n    predicted_labels.extend(predicted.cpu().numpy())\n\n# Calculate confusion matrix, precision, and recall\nconf_matrix = confusion_matrix(true_labels, predicted_labels)\n\n# Create figure and axis objects with larger size and font size\nfig, ax = plt.subplots(figsize=(12, 10))\nplt.rcParams.update({'font.size': 16})\n\n# Create heatmap of confusion matrix\nim = ax.imshow(conf_matrix, cmap='Blues')\n\n# Add colorbar to heatmap\ncbar = ax.figure.colorbar(im, ax=ax)\n\n# Set tick labels and axis labels with larger font size\nax.set_xticks(np.arange(len(class_names)))\nax.set_yticks(np.arange(len(class_names)))\nax.set_xticklabels(class_names, fontsize=14)\nax.set_yticklabels(class_names, fontsize=14)\nax.set_xlabel('Predicted label', fontsize=16)\nax.set_ylabel('True label', fontsize=16)\n\n# Rotate tick labels and set alignment\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n# Add text annotations to heatmap\nfor i in range(len(class_names)):\n    for j in range(len(class_names)):\n        if conf_matrix[i, j] >= -1:  # Modify threshold value as needed\n            text = ax.text(j, i, conf_matrix[i, j],\n                           ha=\"center\", va=\"center\", color=\"y\", fontsize=16)\n        else:\n            text = ax.text(j, i, \"\",\n                           ha=\"center\", va=\"center\", color=\"y\")\n\n# Add title to plot with larger font size\nax.set_title(\"Confusion matrix\", fontsize=20)\n\n# Show plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T12:37:50.138299Z","iopub.status.idle":"2023-08-05T12:37:50.139047Z","shell.execute_reply.started":"2023-08-05T12:37:50.138807Z","shell.execute_reply":"2023-08-05T12:37:50.138830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.prep_model_for_conversion(input_size=[1, 3, config.INPUT_HEIGHT, config.INPUT_WIDTH])\n# Convert model to onnx\ntorch.onnx.export(model, dummy_input,  \"yolo_nas_m.onnx\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"@misc{supergradients,\n  doi = {10.5281/ZENODO.7789328},\n  url = {https://zenodo.org/record/7789328},\n  author = {Aharon,  Shay and {Louis-Dupont} and {Ofri Masad} and Yurkova,  Kate and {Lotem Fridman} and {Lkdci} and Khvedchenya,  Eugene and Rubin,  Ran and Bagrov,  Natan and Tymchenko,  Borys and Keren,  Tomer and Zhilko,  Alexander and {Eran-Deci}},\n  title = {Super-Gradients},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  year = {2021},\n}","metadata":{}}]}