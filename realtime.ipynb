{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is logged into C:\\Users\\Naufal\\sg_logs\\console.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-14 21:00:13] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n",
      "[2023-08-14 21:00:15] WARNING - redirects.py - NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "[2023-08-14 21:00:32] WARNING - __init__.py - Failed to import pytorch_quantization\n",
      "[2023-08-14 21:00:32] WARNING - calibrator.py - Failed to import pytorch_quantization\n",
      "[2023-08-14 21:00:32] WARNING - export.py - Failed to import pytorch_quantization\n",
      "[2023-08-14 21:00:32] WARNING - selective_quantization_utils.py - Failed to import pytorch_quantization\n",
      "[2023-08-14 21:00:32] WARNING - env_sanity_check.py - \u001b[31mFailed to verify operating system: Deci officially supports only Linux kernels. Some features may not work as expected.\u001b[0m\n",
      "[2023-08-14 21:00:32] INFO - env_sanity_check.py - Library check is not supported when super_gradients installed through \"git+https://github.com/...\" command\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t– MATPLOTLIB VERSION: 3.6.3\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import super_gradients\n",
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import Trainer\n",
    "from super_gradients.training import training_hyperparams\n",
    "from super_gradients.training.metrics.classification_metrics import Accuracy, Top5\n",
    "from super_gradients.training.utils.early_stopping import EarlyStop\n",
    "from super_gradients.training import models\n",
    "from super_gradients.training.utils.callbacks import Phase\n",
    "from super_gradients.training.utils.checkpoint_utils import load_checkpoint_to_model\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "# Visualization Imports\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as patches\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm; tqdm.pandas();\n",
    "import plotly.express as px\n",
    "\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageEnhance; Image.MAX_IMAGE_PIXELS = 5_000_000_000;\n",
    "import matplotlib; print(f\"\\t\\t– MATPLOTLIB VERSION: {matplotlib.__version__}\");\n",
    "from matplotlib import animation, rc; rc('animation', html='jshtml')\n",
    "import plotly\n",
    "import PIL\n",
    "import cv2\n",
    "\n",
    "import plotly.io as pio\n",
    "print(pio.renderers)\n",
    "import gc\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "# Visualization Imports\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as patches\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm; tqdm.pandas();\n",
    "import plotly.express as px\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    # specify the paths to datasets\n",
    "\n",
    "    TRAIN_DIR = \"/kaggle/input/fruit-and-vegetable-image-recognition/train\"\n",
    "    TEST_DIR = \"/kaggle/input/fruit-and-vegetable-image-recognition/test\"\n",
    "    VAL_DIR = '/kaggle/input/fruit-and-vegetable-image-recognition/validation'\n",
    "\n",
    "    # set the input height and width\n",
    "    INPUT_HEIGHT = 256\n",
    "    INPUT_WIDTH = 256\n",
    "\n",
    "    # set the input heig/ht and width\n",
    "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "    IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    BATCH_SIZE = 128\n",
    "    MODEL_NAME = 'regnetY800'\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    TRAINING_PARAMS = 'training_hyperparams/default_train_params'\n",
    "    \n",
    "\n",
    "\n",
    "    CHECKPOINT_DIR = 'checkpoints'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-14 21:01:57] INFO - checkpoint_utils.py - Successfully loaded model weights from model.pth checkpoint.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from super_gradients.training  import models\n",
    "from super_gradients.common.object_names import Models\n",
    "\n",
    "#Load weight\n",
    "model = models.get(config.MODEL_NAME, num_classes = 36, checkpoint_path=\"model.pth\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all labels\n",
    "class_names = ['pomegranate', 'pineapple', 'kiwi' ,'peas', 'lettuce', 'carrot' ,'mango',\n",
    " 'paprika', 'raddish', 'apple', 'garlic', 'cauliflower', 'banana' ,'beetroot',\n",
    " 'chilli pepper', 'spinach', 'watermelon', 'potato', 'turnip', 'ginger',\n",
    " 'sweetcorn', 'lemon', 'eggplant', 'sweetpotato', 'capsicum', 'cucumber',\n",
    " 'jalepeno', 'corn', 'onion', 'soy beans', 'bell pepper', 'orange', 'cabbage',\n",
    " 'tomato', 'pear', 'grapes']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline for image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(config.INPUT_HEIGHT,config.INPUT_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=config.IMAGENET_MEAN,\n",
    "                            std=config.IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "def predict(img):\n",
    "    color_converted = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    pil_image=Image.fromarray(color_converted)\n",
    "    transformed_image = transform(pil_image)\n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture an image from the camera\n",
    "    res, frame = cap.read()\n",
    "    \n",
    "    # Preprocess the image\n",
    "    color_converted = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image=Image.fromarray(color_converted)\n",
    "    transformed_image = transform(pil_image)\n",
    "    model.to(config.DEVICE)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
    "        transformed_image = transformed_image.unsqueeze(dim=0)\n",
    "\n",
    "        # make a prediction on image with an extra dimension and send it to the target device\n",
    "        target_image_pred = model(transformed_image.to(config.DEVICE))\n",
    "    # Probabilities\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "    # convert prediction probabilities -> prediction labels\n",
    "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "\n",
    "  \n",
    "\n",
    "    # Classify the image\n",
    "    height, width , channel = frame.shape\n",
    "    sub_img = frame[0:int(height/6),0:int(width)]\n",
    "    black_rect = np.ones(sub_img.shape, dtype=np.uint8)*0\n",
    "    res = cv2.addWeighted(sub_img, 0.77, black_rect,0.23, 0)\n",
    "    FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    FONT_SCALE = 0.8\n",
    "    FONT_THICKNESS = 2\n",
    "    lable_color = (10, 10, 255)\n",
    "    lable = \"Realtime Fruit Classification\"\n",
    "    lable_dimension = cv2.getTextSize(lable,FONT ,FONT_SCALE,FONT_THICKNESS)[0]\n",
    "    textX = int((res.shape[1] - lable_dimension[0]) / 2)\n",
    "    textY = int((res.shape[0] + lable_dimension[1]) / 2)\n",
    "    cv2.putText(res, \"Label: {}\".format(class_names[target_image_pred_label]), (0,textY+22+5), FONT,0.7, lable_color,2)\n",
    "    lable_violation = 'Prob: {}'.format(str(target_image_pred_probs.max())+ \"%\")\n",
    "    violation_text_dimension = cv2.getTextSize(lable_violation,FONT,FONT_SCALE,FONT_THICKNESS )[0]\n",
    "    violation_x_axis = int(res.shape[1]- violation_text_dimension[0])\n",
    "    cv2.putText(res, lable_violation, (violation_x_axis,textY+22+5), FONT,0.7, lable_color,2)\n",
    "    frame[0:int(height/6),0:int(width)] =res\n",
    "    cv2.imshow(\"Sugeng Classification\", frame)\n",
    "\n",
    "    # Press `q` to quit\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
